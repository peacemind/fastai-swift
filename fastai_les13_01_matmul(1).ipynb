{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fastai-les13-01-matmul.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "swift",
      "display_name": "Swift"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZRlD4utdPuX",
        "colab_type": "code",
        "outputId": "3160c98e-123b-4c4d-ef08-0020978087f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        }
      },
      "source": [
        "%system git clone https://github.com/fastai/course-v3.git && mv course-v3/nbs/swift/* /content\n",
        "%install-location $cwd/swift-install\n",
        "%install '.package(path: \"$cwd/FastaiNotebook_00_load_data\")' FastaiNotebook_00_load_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'course-v3'...\n",
            "Installing packages:\n",
            "\t.package(path: \"/content/FastaiNotebook_00_load_data\")\n",
            "\t\tFastaiNotebook_00_load_data\n",
            "With SwiftPM flags: []\n",
            "Working in: /tmp/tmpe4gh29pl/swift-install\n",
            "Fetching https://github.com/mxcl/Path.swift\n",
            "Fetching https://github.com/saeta/Just\n",
            "Fetching https://github.com/latenitesoft/NotebookExport\n",
            "Completed resolution in 4.11s\n",
            "Cloning https://github.com/latenitesoft/NotebookExport\n",
            "Resolving https://github.com/latenitesoft/NotebookExport at 0.6.0\n",
            "Cloning https://github.com/mxcl/Path.swift\n",
            "Resolving https://github.com/mxcl/Path.swift at 0.16.3\n",
            "Cloning https://github.com/saeta/Just\n",
            "Resolving https://github.com/saeta/Just at 0.7.3\n",
            "[1/11] Compiling Path Extensions.swift\n",
            "[2/11] Compiling Path Path+Attributes.swift\n",
            "[3/11] Compiling Path Path+Codable.swift\n",
            "[4/11] Compiling Path Path+CommonDirectories.swift\n",
            "[5/11] Compiling Path Path+FileManager.swift\n",
            "[6/11] Compiling Path Path+StringConvertibles.swift\n",
            "[7/11] Compiling Path Path+ls.swift\n",
            "[8/11] Compiling Path Path->Bool.swift\n",
            "[9/11] Compiling Path Path.swift\n",
            "[10/12] Merging module Path\n",
            "[11/16] Compiling NotebookExport DependencyDescription.swift\n",
            "[13/16] Compiling NotebookExport ExtensionUtils.swift\n",
            "[14/17] Merging module Just\n",
            "[15/17] Compiling NotebookExport NotebookExport.swift\n",
            "[16/17] Compiling NotebookExport PackageManifest.swift\n",
            "[17/18] Merging module NotebookExport\n",
            "[18/19] Compiling FastaiNotebook_00_load_data 00_load_data.swift\n",
            "[19/20] Merging module FastaiNotebook_00_load_data\n",
            "[20/21] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
            "[21/22] Merging module jupyterInstalledPackages\n",
            "[22/22] Linking libjupyterInstalledPackages.so\n",
            "Initializing Swift...\n",
            "Installation complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3dN6ITs1H5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "//export\n",
        "import Path\n",
        "import TensorFlow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PanlmR31L25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import FastaiNotebook_00_load_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHboxEpS1RaO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Get some Tensors to play with\n",
        "\n",
        "We can initialize a tensor in lots of different ways because in swift, two functions with the same name can coexist as long as they don't have the same signatures. Different named arguments give different signatures, so all of those are different init functions of Tensor:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHtjzecl1QAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let zeros = Tensor<Float>(zeros: [1,4,5])\n",
        "let ones = Tensor<Float>(ones: [12,4,5])\n",
        "let twos = Tensor<Float>(repeating: 2.0, shape:[2,3,4,5])\n",
        "let range = Tensor<Float>(rangeFrom: 0, to: 32, stride: 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJW0d3cL2B8_",
        "colab_type": "text"
      },
      "source": [
        "Those are just some examples and there are many more! Here we grab random numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDNH0XCS11Ro",
        "colab_type": "code",
        "outputId": "bb2017e5-0aa2-414c-eaf5-e23bcf510a34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "let xTrain = Tensor<Float>(randomNormal: [5, 784])\n",
        "var weights = Tensor<Float>(randomNormal: [784, 10]) / sqrt(784)\n",
        "print(weights[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ -0.055401582,    0.04527598,   0.056512393,   -0.07000102,   0.009345101,   0.060234334,\r\n",
            " 0.00037808318, -0.0012498533,   0.029811794,  -0.015313621]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD4MgMvv2grA",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Building Matmul\n",
        "\n",
        "Ok, now that we know how floating point types and arrays work, we can finally build our own matmul from scratch, using a few loops. We will take the two input matrices as single dimensional arrays so we can show manual indexing into them, the hard way:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crjmhT9a2Z23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "// a and b are the flattened array elements, aDims/bDims are the #rows/columns of the arrays.\n",
        "func swiftMatmul(a: [Float], b: [Float], aDims: (Int,Int), bDims: (Int,Int)) -> [Float] {\n",
        "    assert(aDims.1 == bDims.0, \"matmul shape mismatch\")\n",
        "    \n",
        "    var res = Array(repeating: Float(0.0), count: aDims.0 * bDims.1)\n",
        "    for i in 0 ..< aDims.0 {\n",
        "        for j in 0 ..< bDims.1 {\n",
        "            for k in 0 ..< aDims.1 {\n",
        "                res[i*bDims.1+j] += a[i*aDims.1+k] * b[k*bDims.1+j]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return res\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KIMSfQK3Owr",
        "colab_type": "text"
      },
      "source": [
        "To try this out, we extract the scalars out of our MNIST data as an array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6gBL0El3PLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let flatA = xTrain[0..<5].scalars\n",
        "let flatB = weights.scalars\n",
        "let (aDims,bDims) = ((5, 784), (784, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilmXde6j4Tq_",
        "colab_type": "text"
      },
      "source": [
        "Now that we've got everything together, we can try it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrGTNruB4SJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "var resultArray = swiftMatmul(a: flatA, b: flatB, aDims: aDims, bDims: bDims)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEur2TsZ4rA-",
        "colab_type": "code",
        "outputId": "e69862dc-6e0d-4d0b-958b-7e2d3c9dce68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "time(repeating: 100) {\n",
        "    _ = swiftMatmul(a: flatA, b: flatB, aDims: aDims, bDims: bDims)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average: 0.14033041999999996 ms,   min: 0.114721 ms,   max: 0.409443 ms\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmV73bXa4wfI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Awesome, that is pretty fast - compare that to 835 ms with Python!\n",
        "\n",
        "You might be wondering what that time(repeating:) builtin is. As you might guess, this is actually a Swift function - one that is using \"trailing closure\" syntax to specify the body of the timing block. Trailing closures are passed as arguments to the function, and in this case, the function was defined in our ✅00_load_data workbook. Let's take a look!\n",
        "\n",
        "Getting the performance of C 💯\n",
        "\n",
        "This performance is pretty great, but we can do better. Swift is a memory safe language (like Python), which means it has to do array bounds checks and some other stuff. Fortunately, Swift is a pragmatic language that allows you to drop through this to get peak performance - check out Jeremy's article High Performance Numeric Programming with Swift: Explorations and Reflections for a deep dive.\n",
        "\n",
        "One thing you can do is use UnsafePointer (which is basically a raw C pointer) instead of using a bounds checked array. This isn't memory safe, but gives us about a 2x speedup in this case!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB-35F8d4u2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "// a and b are the flattened array elements, aDims/bDims are the #rows/columns of the arrays.\n",
        "func swiftMatmulUnsafe(a: UnsafePointer<Float>, b: UnsafePointer<Float>, aDims: (Int,Int), bDims: (Int,Int)) -> [Float] {\n",
        "    assert(aDims.1 == bDims.0, \"matmul shape mismatch\")\n",
        "    \n",
        "    var res = Array(repeating: Float(0.0), count: aDims.0 * bDims.1)\n",
        "    res.withUnsafeMutableBufferPointer { res in \n",
        "        for i in 0 ..< aDims.0 {\n",
        "            for j in 0 ..< bDims.1 {\n",
        "                for k in 0 ..< aDims.1 {\n",
        "                    res[i*bDims.1+j] += a[i*aDims.1+k] * b[k*bDims.1+j]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return res\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZT2VyJq5TOk",
        "colab_type": "code",
        "outputId": "c6aeeef4-d582-4545-c928-872671b07311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "time(repeating: 100) {\n",
        "    _ = swiftMatmulUnsafe(a: flatA, b: flatB, aDims: aDims, bDims: bDims)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average: 0.05983414000000002 ms,   min: 0.053706 ms,   max: 0.09684 ms\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGvnQf6F5XgS",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "One of the other cool things about this is that we can provide a nice idiomatic API to the caller of this, and keep all the unsafe shenanigans inside the implementation of this function.\n",
        "\n",
        "If you really want to fall down the rabbit hole, you can look at the implementation of UnsafePointer, which is of written in Swift wrapping LLVM pointer operations. This means you can literally get the performance of C code directly in Swift, while providing easy to use high level APIs!\n",
        "\n",
        "Swift 💖 C APIs too: you get the full utility of the C ecosystem\n",
        "\n",
        "Swift even lets you transparently work with C APIs, just like it does with Python. This can be used for both good and evil. For example, here we directly call the malloc function, dereference the uninitialized pointer, and print it out:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzIFBf-N5Uyh",
        "colab_type": "code",
        "outputId": "336de2ac-f418-48fa-cb54-7e6c5354c560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import Glibc\n",
        "\n",
        "let ptr : UnsafeMutableRawPointer = malloc(42)\n",
        "\n",
        "print(\"☠️☠️ Uninitialized garbage =\", ptr.load(as: UInt8.self))\n",
        "\n",
        "free(ptr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "☠️☠️ Uninitialized garbage = 240\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z_LEazP5rVC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "An UnsafeMutableRawPointer (implementation) isn't something you should use lightly, but when you work with C APIs, you'll see various types like that in the function signatures.\n",
        "\n",
        "Calling malloc and free directly aren't recommended in Swift, but is useful and important when you're working with C APIs that expect to get malloc'd memory, which comes up when you're written a safe Swift wrapper for some existing code.\n",
        "\n",
        "Speaking of existing code, let's take a look at that Python interop we touched on before:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-BLIrJd5oTB",
        "colab_type": "code",
        "outputId": "02c91e33-67a6-4f7d-d0d3-06283ae48e73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "import Python\n",
        "let np = Python.import(\"numpy\")\n",
        "let pickle = Python.import(\"pickle\")\n",
        "let sys = Python.import(\"sys\")\n",
        "\n",
        "print(\"🐍list =    \", pickle.dumps([1, 2, 3]))\n",
        "print(\"🐍ndarray = \", pickle.dumps(np.array([[1, 2], [3, 4]])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🐍list =     b'\\x80\\x03]q\\x00(K\\x01K\\x02K\\x03e.'\r\n",
            "🐍ndarray =  b'\\x80\\x03cnumpy.core.multiarray\\n_reconstruct\\nq\\x00cnumpy\\nndarray\\nq\\x01K\\x00\\x85q\\x02C\\x01bq\\x03\\x87q\\x04Rq\\x05(K\\x01K\\x02K\\x02\\x86q\\x06cnumpy\\ndtype\\nq\\x07X\\x02\\x00\\x00\\x00i8q\\x08K\\x00K\\x01\\x87q\\tRq\\n(K\\x03X\\x01\\x00\\x00\\x00<q\\x0bNNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00tq\\x0cb\\x89C \\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00q\\rtq\\x0eb.'\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ul1t8db6KIB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Of course this is all written in Swift as well. You can probably guess how this works now: PythonObject is a Swift struct that wraps a pointer to the Python interpreter's notion of a Python object.\n",
        "\n",
        "@dynamicCallable\n",
        "\n",
        "@dynamicMemberLookup\n",
        "\n",
        "public struct PythonObject {\n",
        "\n",
        "  var reference: PyReference\n",
        "  \n",
        "  ...\n",
        "  \n",
        "}\n",
        "\n",
        "The @dynamicMemberLookup attribute allows it to dynamically handle all member lookups (like x.y) by calling into the PyObject_GetAttrString runtime call. Similarly, the @dynamicCallable attribute allows the type to intercept all calls to a PythonObject (like x()), which it implements using the PyObject_Call runtime call.\n",
        "\n",
        "Because Swift has such simple and transparent access to C, it allows building very nice first-class Swift APIs that talk directly to the lower level implementation, and these implementations can have very little overhead.\n",
        "\n",
        "Working with Tensor\n",
        "\n",
        "Lets get back into matmul and explore more of the Tensor type as provided by the TensorFlow module. You can see all things Tensor can do in the official documentation.\n",
        "\n",
        "Here are some highlights. We saw how you can get zeros or random data:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VstxaExM54hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "var bias = Tensor<Float>(zeros: [10])\n",
        "\n",
        "let m1 = Tensor<Float>(randomNormal: [5, 784])\n",
        "let m2 = Tensor<Float>(randomNormal: [784, 10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55OATgaoOfqn",
        "colab_type": "text"
      },
      "source": [
        "Tensor carry data and a shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwTjyGw6Oemb",
        "colab_type": "code",
        "outputId": "623c0dff-9ec4-4ee8-89a0-9029b0446029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(\"m1: \", m1.shape)\n",
        "print(\"m2: \", m2.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "m1:  [5, 784]\r\n",
            "m2:  [784, 10]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIi_g9BFOrfT",
        "colab_type": "text"
      },
      "source": [
        "The Tensor type provides all the normal stuff you'd expect as methods. Including arithmetic, convolutions, etc and this includes full support for broadcasting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hR7w5pPOo94",
        "colab_type": "code",
        "outputId": "eddace05-27e2-4419-d010-23ad8ab8c437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "let small = Tensor<Float>([[1, 2],\n",
        "                           [3, 4]])\n",
        "\n",
        "print(\"🔢2x2:\\n\", small)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "🔢2x2:\r\n",
            " [[1.0, 2.0],\r\n",
            " [3.0, 4.0]]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng1AU5J2O-9y",
        "colab_type": "text"
      },
      "source": [
        "MatMul Operator: In addition to using the global matmul(a, b) function, you can also use the a • b operator to matmul together two things. This is just like the @ operator in Python. You can get it with the option-8 on Mac or compose-.-= elsewhere. Or if you prefer, just use the matmul() function we've seen already."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2j4kKvKO8M4",
        "colab_type": "code",
        "outputId": "70b094c6-b0ce-4ccb-fb5c-c2ef36bfb449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "print(\"⊞ matmul:\\n\",  matmul(small, small))\n",
        "print(\"\\n⊞ again:\\n\", small • small)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "⊞ matmul:\r\n",
            " [[ 7.0, 10.0],\r\n",
            " [15.0, 22.0]]\r\n",
            "\r\n",
            "⊞ again:\r\n",
            " [[ 7.0, 10.0],\r\n",
            " [15.0, 22.0]]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69TY2rqFPKv6",
        "colab_type": "text"
      },
      "source": [
        "Reshaping works the way you'd expect:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AleFO7NPIA5",
        "colab_type": "code",
        "outputId": "1355b8fc-1970-4d9e-993b-882da2d6a49c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "var m = Tensor([1.0, 2, 3, 4, 5, 6, 7, 8, 9]).reshaped(to: [3, 3])\n",
        "print(m)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0, 2.0, 3.0],\r\n",
            " [4.0, 5.0, 6.0],\r\n",
            " [7.0, 8.0, 9.0]]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7UE8YvTPiIL",
        "colab_type": "text"
      },
      "source": [
        "You have the basic mathematical functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BouUM-lQPboP",
        "colab_type": "code",
        "outputId": "73271fba-3614-40a4-ee84-5dd03b0bd5ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sqrt((m * m).sum())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16.881943016134134\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBKHR68jPmxE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Elementwise ops and comparisons\n",
        "\n",
        "Standard math operators (+,-,*,/) are all element-wise, and there are a bunch of standard math functions like sqrt and pow. Here are some examples:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N7-ItBGPlHO",
        "colab_type": "code",
        "outputId": "f4afbcef-12ef-44cf-db86-aeb54161de10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "var a = Tensor([10.0, 6, -4])\n",
        "var b = Tensor([2.0, 8, 7])\n",
        "(a,b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "▿ 2 elements\n",
              "  - .0 : [10.0,  6.0, -4.0]\n",
              "  - .1 : [2.0, 8.0, 7.0]\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGr5nPbYPzDH",
        "colab_type": "code",
        "outputId": "00e25e6a-59a8-46c0-8bc4-1751b418621c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(\"add: \", a + b)\n",
        "print(\"mul: \", a * b)\n",
        "print(\"sqrt: \", sqrt(a))\n",
        "print(\"pow: \", pow(a, b))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "add:  [12.0, 14.0,  3.0]\r\n",
            "mul:  [ 20.0,  48.0, -28.0]\r\n",
            "sqrt:  [3.1622776601683795,  2.449489742783178,               -nan]\r\n",
            "pow:  [    100.0, 1679616.0,  -16384.0]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbiwFAPsQDmF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Comparison operators (>,<,==,!=,...) in Swift are supposed to return a single Bool value, so they are true if all the elements of the tensors satisfy the comparison.\n",
        "\n",
        "Elementwise versions have the . prefix, which is read as \"pointwise\": .>, .<, .==, etc. You can merge a tensor of bools into a single Bool with the any() and all() methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDhRphZOQCTG",
        "colab_type": "code",
        "outputId": "9f4df8f7-7c88-4db7-ed95-3a1c79bc0958",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "a < b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "false\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l3tHm0PQGJx",
        "colab_type": "code",
        "outputId": "784ed0e2-1e1c-42fc-82f7-4922b9a64b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "a .< b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[false,  true,  true]\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcIU2B4GQIi2",
        "colab_type": "code",
        "outputId": "fba5f332-655b-45f7-b32f-75cab807a2dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print((a .> 0).all())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "false\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbp90LkFQdZn",
        "colab_type": "code",
        "outputId": "9b21d7da-f587-4125-e369-d3d5096c77a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print((a .> 0).any())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P2-TYI-Qji6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Broadcasting\n",
        "\n",
        "Broadcasting with a scalar works just like in Python:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3uGcUuAQiWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "var a = Tensor([10.0, 6.0, -4.0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtUrhvVnQp9d",
        "colab_type": "code",
        "outputId": "9c529fe9-e165-44fc-8c4f-c985008b9f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(a+1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[11.0,  7.0, -3.0]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9O4vsdfQsCi",
        "colab_type": "code",
        "outputId": "74e398fe-ef2e-416a-f44a-754e7803ab82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "2 * m"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[ 2.0,  4.0,  6.0],\n",
              " [ 8.0, 10.0, 12.0],\n",
              " [14.0, 16.0, 18.0]]\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XIKXoDRQvb9",
        "colab_type": "text"
      },
      "source": [
        "Broadcasting a vector with a matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqYedqE-Qtc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let c = Tensor([10.0, 20.0, 30.0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMDBNxM0Q4do",
        "colab_type": "text"
      },
      "source": [
        "By default, broadcasting is done by adding 1 dimensions to the beginning until dimensions of both objects match."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-ytQq-bQ1Tt",
        "colab_type": "code",
        "outputId": "a12116ae-07a9-44b6-878b-a78c1be9e3b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "m + c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[11.0, 22.0, 33.0],\n",
              " [14.0, 25.0, 36.0],\n",
              " [17.0, 28.0, 39.0]]\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEnJt5doQ6Nu",
        "colab_type": "code",
        "outputId": "df6bc351-1c9a-448b-d0be-0f3a6171780b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "c + m"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[11.0, 22.0, 33.0],\n",
              " [14.0, 25.0, 36.0],\n",
              " [17.0, 28.0, 39.0]]\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVxkbS6wRJLg",
        "colab_type": "text"
      },
      "source": [
        "To broadcast on the other dimensions, one has to use expandingShape to add the dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qignx5t5RE9t",
        "colab_type": "code",
        "outputId": "b144cb08-c402-42a8-b435-718f051744e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "m + c.expandingShape(at: 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[11.0, 12.0, 13.0],\n",
              " [24.0, 25.0, 26.0],\n",
              " [37.0, 38.0, 39.0]]\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVXdsohUROOr",
        "colab_type": "code",
        "outputId": "e524b9fd-3691-4a52-cabe-2da5c94d3938",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "c.expandingShape(at: 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[10.0],\n",
              " [20.0],\n",
              " [30.0]]\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNr8u8TkTnF_",
        "colab_type": "text"
      },
      "source": [
        "Broadcasting rules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7HEJoyoRfUk",
        "colab_type": "code",
        "outputId": "ca1fe637-a569-4028-b2fa-df1c32f7a3dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(c.expandingShape(at: 0).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 3]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIWGGF6HTscF",
        "colab_type": "code",
        "outputId": "814760b2-6543-4805-bd73-a961fac5a39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "c.expandingShape(at: 0) * c.expandingShape(at: 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[100.0, 200.0, 300.0],\n",
              " [200.0, 400.0, 600.0],\n",
              " [300.0, 600.0, 900.0]]\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zakgJasYTvvS",
        "colab_type": "code",
        "outputId": "ce79fb8f-c343-4c18-bfc9-5410721c8c08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "c.expandingShape(at: 0) .> c.expandingShape(at: 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[false,  true,  true],\n",
              " [false, false,  true],\n",
              " [false, false, false]]\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln-CWiM1bB0S",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Matmul using Tensor\n",
        "\n",
        "Coming back to our matmul algorithm, we can implement exactly what we had before by using subscripting into a tensor, instead of subscripting into an array. Let's see how that works:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsR1ddxRT37v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func tensorMatmul(_ a: Tensor<Float>, _ b: Tensor<Float>) -> Tensor<Float> {\n",
        "    var res = Tensor<Float>(zeros: [a.shape[0], b.shape[1]])\n",
        "\n",
        "    for i in 0 ..< a.shape[0] {\n",
        "        for j in 0 ..< b.shape[1] {\n",
        "            for k in 0 ..< a.shape[1] {\n",
        "                res[i, j] += a[i, k] * b[k, j]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return res\n",
        "}\n",
        "\n",
        "_ = tensorMatmul(m1, m2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKi0-i5gbJpi",
        "colab_type": "code",
        "outputId": "3be526b5-bf27-4ab4-d90c-a499be9d449f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "time { \n",
        "    let tmp = tensorMatmul(m1, m2)\n",
        "    \n",
        "    // Copy a scalar back to the host to force a GPU sync.\n",
        "    _ = tmp[0, 0].scalar\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optional(7.3641605)\r\n",
            "average: 2917.642314 ms,   min: 2917.642314 ms,   max: 2917.642314 ms\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzIFOr_KbdGX",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "What, what just happened?? We used to be less than a tenth of a millisecond, now we're taking multiple seconds. It turns out that Tensor's are very good at bulk data processing, but they are not good at doing one float at a time. Make sure to use the coarse-grained operations. We can make this faster by vectorizing each loop in turn.\n",
        "\n",
        "Slides: Granularity of Tensor Operations.\n",
        "\n",
        "Vectorize the inner loop into a multiply + sum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQCG8h6sbT9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func elementWiseMatmul(_ a:Tensor<Float>, _ b:Tensor<Float>) -> Tensor<Float>{\n",
        "    let (ar, ac) = (a.shape[0], a.shape[1])\n",
        "    let (br, bc) = (b.shape[0], b.shape[1])\n",
        "    var res = Tensor<Float>(zeros: [ac, br])\n",
        "    \n",
        "    for i in 0 ..< ar {\n",
        "        let row = a[i]\n",
        "        for j in 0 ..< bc {\n",
        "            res[i, j] = (row * b.slice(lowerBounds: [0,j], upperBounds: [ac,j+1]).squeezingShape(at: 1)).sum()\n",
        "        }\n",
        "    }\n",
        "    return res\n",
        "}\n",
        "\n",
        "_ = elementWiseMatmul(m1, m2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxY6uLdLbvJN",
        "colab_type": "code",
        "outputId": "b761f045-e2f4-4213-e73c-5299112c6159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "time { \n",
        "    let tmp = elementWiseMatmul(m1, m2)\n",
        "\n",
        "    // Copy a scalar back to the host to force a GPU sync.\n",
        "    _ = tmp[0, 0].scalar\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average: 376.989332 ms,   min: 376.989332 ms,   max: 376.989332 ms\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLIzaefzb8OH",
        "colab_type": "text"
      },
      "source": [
        "Vectorize the inner two loops with broadcasting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4hqnO-abxVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func broadcastMatmult(_ a:Tensor<Float>, _ b:Tensor<Float>) -> Tensor<Float>{\n",
        "    var res = Tensor<Float>(zeros: [a.shape[0], b.shape[1]])\n",
        "    for i in 0..<a.shape[0] {\n",
        "        res[i] = (a[i].expandingShape(at: 1) * b).sum(squeezingAxes: 0)\n",
        "    }\n",
        "    return res\n",
        "}\n",
        "\n",
        "_ = broadcastMatmult(m1, m2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heyd_pwFcGGu",
        "colab_type": "code",
        "outputId": "545ccfe2-cd8f-4634-8673-d32735666599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "time(repeating: 100) {\n",
        "    let tmp = broadcastMatmult(m1, m2)\n",
        "\n",
        "    // Copy a scalar back to the host to force a GPU sync.\n",
        "    _ = tmp[0, 0].scalar\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average: 0.4759021600000001 ms,   min: 0.396759 ms,   max: 0.649301 ms\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Nf-AtHOcRQE",
        "colab_type": "text"
      },
      "source": [
        "Vectorize the whole thing with one Tensorflow op"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3UohCvScIJU",
        "colab_type": "code",
        "outputId": "b7453146-71cb-42b4-d865-d37715b2e8f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "time(repeating: 100) { _ = m1 • m2 }"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average: 0.017368049999999996 ms,   min: 0.014872 ms,   max: 0.063304 ms\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRSzAkrGcbBi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Ok, now that we have matmul, we can continue to build out our framework.\n",
        "\n",
        "To complete today's lesson, let's jump way way up the stack to see Workbook 11.\n",
        "\n",
        "Tensorflow vectorizes, parallelizes, and scales\n",
        "\n",
        "The reason that TensorFlow works in practice is that it can scale way up to large matrices, for example, lets try some thing a bit larger:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0_nOyL8cUPl",
        "colab_type": "code",
        "outputId": "70aa7b19-a50b-482c-cc6e-93d623713dc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "func timeMatmulTensor(size: Int) {\n",
        "    var matrix = Tensor<Float>(randomNormal: [size, size])\n",
        "    print(\"\\n\\(size)x\\(size):\\n  ⏰\", terminator: \"\")\n",
        "    time(repeating: 10) { \n",
        "        let matrix = matrix • matrix \n",
        "        _ = matrix[0, 0].scalar\n",
        "    }\n",
        "}\n",
        "\n",
        "timeMatmulTensor(size: 1)     // Tiny\n",
        "timeMatmulTensor(size: 10)    // Bigger\n",
        "timeMatmulTensor(size: 100)   // Even Bigger\n",
        "timeMatmulTensor(size: 1000)  // Biggerest\n",
        "timeMatmulTensor(size: 5000)  // Even Biggerest"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r\n",
            "1x1:\r\n",
            "  ⏰average: 0.0371017 ms,   min: 0.026589 ms,   max: 0.081339 ms\r\n",
            "\r\n",
            "10x10:\r\n",
            "  ⏰average: 0.0326963 ms,   min: 0.028673 ms,   max: 0.044256 ms\n",
            "\n",
            "100x100:\n",
            "  ⏰average: 0.1867425 ms,   min: 0.090954 ms,   max: 0.35335 ms\n",
            "\n",
            "1000x1000:\n",
            "  ⏰average: 34.32726459999999 ms,   min: 33.071799 ms,   max: 37.019984 ms\n",
            "\n",
            "5000x5000:\n",
            "  ⏰average: 3779.2534528000006 ms,   min: 3699.616346 ms,   max: 3914.151122 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNLi76C5crZ7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "In constrast, our simple CPU implementation takes a lot longer to do the same work. For example:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diQC4F5YcnSN",
        "colab_type": "code",
        "outputId": "d784637f-fb37-48e1-8edb-a0d27fb9bc6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "func timeMatmulSwift(size: Int, repetitions: Int = 10) {\n",
        "    var matrix = Tensor<Float>(randomNormal: [size, size])\n",
        "    let matrixFlatArray = matrix.scalars\n",
        "\n",
        "    print(\"\\n\\(size)x\\(size):\\n  ⏰\", terminator: \"\")\n",
        "    time(repeating: repetitions) { \n",
        "       _ = swiftMatmulUnsafe(a: matrixFlatArray, b: matrixFlatArray, aDims: (size,size), bDims: (size,size))\n",
        "    }\n",
        "}\n",
        "\n",
        "timeMatmulSwift(size: 1)     // Tiny\n",
        "timeMatmulSwift(size: 10)    // Bigger\n",
        "timeMatmulSwift(size: 100)   // Even Bigger\n",
        "timeMatmulSwift(size: 1000, repetitions: 1)  // Biggerest\n",
        "\n",
        "print(\"\\n5000x5000: skipped, it takes tooo long!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r\n",
            "1x1:\r\n",
            "  ⏰average: 0.00018449999999999999 ms,   min: 0.000161 ms,   max: 0.000331 ms\r\n",
            "\r\n",
            "10x10:\r\n",
            "  ⏰average: 0.0019013000000000003 ms,   min: 0.001861 ms,   max: 0.001995 ms\n",
            "\n",
            "100x100:\n",
            "  ⏰average: 1.6054541 ms,   min: 1.445917 ms,   max: 1.752149 ms\n",
            "\n",
            "1000x1000:\n",
            "  ⏰average: 2027.680326 ms,   min: 2027.680326 ms,   max: 2027.680326 ms\n",
            "\n",
            "5000x5000: skipped, it takes tooo long!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBFDe6m3c-X_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Why is TensorFlow so so so much faster than our CPU implementation? Well there are two reasons: the first of which is that it uses GPU hardware, which is much faster for math like this. That said, there are a ton of tricks (involving memory hierarchies, cache blocking, and other tricks) that make matrix multiplications go fast on CPUs and other hardware.\n",
        "\n",
        "For example, try using TensorFlow on the CPU to do the same computation as above:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMIgqhfSct7L",
        "colab_type": "code",
        "outputId": "07c8295c-1b05-4dbc-a35f-686da2bb42ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "withDevice(.cpu) {\n",
        "    timeMatmulTensor(size: 1)     // Tiny\n",
        "    timeMatmulTensor(size: 10)    // Bigger\n",
        "    timeMatmulTensor(size: 100)   // Even Bigger\n",
        "    timeMatmulTensor(size: 1000)  // Biggerest\n",
        "    timeMatmulTensor(size: 5000)  // Even Biggerest\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r\n",
            "1x1:\r\n",
            "  ⏰average: 0.07955379999999998 ms,   min: 0.054396 ms,   max: 0.106164 ms\r\n",
            "\r\n",
            "10x10:\r\n",
            "  ⏰average: 0.0653583 ms,   min: 0.034122 ms,   max: 0.107788 ms\n",
            "\n",
            "100x100:\n",
            "  ⏰average: 0.2697404 ms,   min: 0.124307 ms,   max: 0.410538 ms\n",
            "\n",
            "1000x1000:\n",
            "  ⏰average: 32.7230537 ms,   min: 30.802024 ms,   max: 34.217482 ms\n",
            "\n",
            "5000x5000:\n",
            "  ⏰average: 3724.8538655999996 ms,   min: 3706.471326 ms,   max: 3745.51614 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4AakC7wdSkl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "This is a pretty big difference. On my hardware, it takes 2287ms for Swift to do a 1000x1000 multiply on the CPU, it takes TensorFlow 6.7ms to do the same work on the CPU, and takes TensorFlow 0.49ms to do it on a GPU.\n",
        "\n",
        "Hardware Accelerators vs Flexibility\n",
        "\n",
        "One of the big challenges with machine learning frameworks today is that they provide a fixed set of \"ops\" that you can use with high performance. There is a lot of work underway to fix this. The XLA compiler in TensorFlow is an important piece of this, which allows more flexibility in the programming model while still providing high performance by using compilers to target the hardware accelerator. If you're interested in the details, there is a great video by the creator of Halide explaining why this is challenging.\n",
        "\n",
        "TensorFlow internals are undergoing significant changes (slide) including the introduction of the XLA compiler, and the introduction of MLIR compiler technology.\n",
        "\n",
        "Tensor internals and Raw TensorFlow operations\n",
        "\n",
        "TensorFlow provides hundreds of different operators, and they sort of grew organically over time. This means that there are some deprecated operators, they aren't particularly consistent, and there are other oddities. As such, the Tensor type provides a curated set of these operators as methods.\n",
        "\n",
        "Whereas Int and Float are syntactic sugar for LLVM, and PythonObject is syntactic sugar for the Python interpreter, Tensor ends up being syntactic sugar for the TensorFlow operator set. You can dive in and see its implementation in Swift in the S4TF TensorFlow module, e.g.:\n",
        "\n",
        "public struct Tensor<Scalar : TensorFlowScalar> : \n",
        "  \n",
        "  TensorProtocol {\n",
        "  \n",
        "  /// The underlying `TensorHandle`.\n",
        "  \n",
        "  /// - Note: `handle` is public to allow user defined ops, but should not\n",
        "  \n",
        "  /// normally be used otherwise.\n",
        "  \n",
        "  public let handle: TensorHandle<Scalar>\n",
        "  ... \n",
        "}\n",
        "\n",
        "Here we see the internal implementation details of Tensor, which stores a TensorHandle - the internal implementation detail of the TensorFlow Eager runtime.\n",
        "\n",
        "Methods are defined on Tensor just like you'd expect, here is the basic addition operator, defined over all numeric tensors (i.e., not tensors of Bool):\n",
        "\n",
        "extension Tensor : AdditiveArithmetic where Scalar :\n",
        "  \n",
        "  Numeric {\n",
        "  \n",
        "  /// Adds two tensors and produces their sum.\n",
        "  \n",
        "  /// - Note: `+` supports broadcasting.\n",
        "  \n",
        "  public static func + (lhs: Tensor, rhs: Tensor) -> Tensor {\n",
        "  \n",
        "    return Raw.add(lhs, rhs)\n",
        "  \n",
        "  }\n",
        "  \n",
        "}\n",
        "\n",
        "But wait, what is this Raw thing?\n",
        "Raw TensorFlow ops\n",
        "\n",
        "TensorFlow has a database of the operators it defines, which gets encoded into a protocol buffer. From this protobuf, all of the operators automatically get a Raw operator (implemented in terms of a lower level #tfop primitive).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9odBBNVdHzK",
        "colab_type": "code",
        "outputId": "7b0c05ad-c88b-47be-ee33-929af9a67faa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "// Explore the contents of the Raw namespace by typing Raw.<tab>\n",
        "print(Raw.zerosLike(c))\n",
        "\n",
        "// Raw."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0, 0.0, 0.0]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpsFoZ_YeJwp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "There is an entire tutorial on Raw operators on github/TensorFlow/swift. The key thing to know is that TensorFlow can do almost anything, so if there is no obvious method on Tensor to do what you need it is worth checking out the tutorial to see how to do this.\n",
        "\n",
        "As one example, later parts of the tutorial need the ability to load files and decode JPEGs. Swift for TensorFlow doesn't have these as methods on StringTensor yet, but we can add them like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTVO1alIeG5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "//export\n",
        "public extension StringTensor {\n",
        "    // Read a file into a Tensor.\n",
        "    init(readFile filename: String) {\n",
        "        self.init(readFile: StringTensor(filename))\n",
        "    }\n",
        "    init(readFile filename: StringTensor) {\n",
        "        self = Raw.readFile(filename: filename)\n",
        "    }\n",
        "\n",
        "    // Decode a StringTensor holding a JPEG file into a Tensor<UInt8>.\n",
        "    func decodeJpeg(channels: Int = 0) -> Tensor<UInt8> {\n",
        "        return Raw.decodeJpeg(contents: self, channels: Int64(channels), dctMethod: \"\") \n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNAMsg-weOHt",
        "colab_type": "text"
      },
      "source": [
        "Export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw4HQXOaeMfJ",
        "colab_type": "code",
        "outputId": "b5401215-c3c5-45c6-b6ec-e80fe44a5be0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import NotebookExport\n",
        "let exporter = NotebookExport(Path.cwd/\"01_matmul.ipynb\")\n",
        "print(exporter.export(usingPrefix: \"FastaiNotebook_\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "success\r\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}