{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fastai-les14-04-callbacks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "swift",
      "display_name": "Swift"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfA5GKNXTdoQ",
        "colab_type": "text"
      },
      "source": [
        "Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZRlD4utdPuX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "87c328af-afdd-4709-a851-90f501c36c08"
      },
      "source": [
        "%install-location $cwd/swift-install\n",
        "%install '.package(path: \"$cwd/FastaiNotebook_03_minibatch_training\")' FastaiNotebook_03_minibatch_training"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing packages:\n",
            "\t.package(path: \"/content/FastaiNotebook_03_minibatch_training\")\n",
            "\t\tFastaiNotebook_03_minibatch_training\n",
            "With SwiftPM flags: []\n",
            "Working in: /tmp/tmp_5wsbvlq/swift-install\n",
            "Updating https://github.com/mxcl/Path.swift\n",
            "Updating https://github.com/saeta/Just\n",
            "Updating https://github.com/latenitesoft/NotebookExport\n",
            "Completed resolution in 1.36s\n",
            "[1/7] Compiling FastaiNotebook_03_minibatch_training 02_fully_connected.swift\n",
            "[2/7] Compiling FastaiNotebook_03_minibatch_training 02a_why_sqrt5.swift\n",
            "[3/7] Compiling FastaiNotebook_03_minibatch_training 03_minibatch_training.swift\n",
            "[4/7] Compiling FastaiNotebook_03_minibatch_training 00_load_data.swift\n",
            "[5/7] Compiling FastaiNotebook_03_minibatch_training 01_matmul.swift\n",
            "[6/7] Compiling FastaiNotebook_03_minibatch_training 01a_fastai_layers.swift\n",
            "[7/8] Merging module FastaiNotebook_03_minibatch_training\n",
            "[8/9] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
            "[9/10] Merging module jupyterInstalledPackages\n",
            "[10/10] Linking libjupyterInstalledPackages.so\n",
            "Initializing Swift...\n",
            "Installation complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shniREUdTgMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "//export\n",
        "import Path\n",
        "import TensorFlow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WNeGFZWUUFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import FastaiNotebook_03_minibatch_training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdhKGhJTUXmo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Load data\n",
        "\n",
        "We load our data and define a basic model like in the previous notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmG7qalnUVoe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "3d424e69-982b-42cc-eb77-ffd5d281926d"
      },
      "source": [
        "var (xTrain,yTrain,xValid,yValid) = loadMNIST(path: mnistPath, flat: true)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-08-18 01:22:36.023150: W tensorflow/core/framework/allocator.cc:107] Allocation of 188160000 exceeds 10% of system memory.\n",
            "2019-08-18 01:22:36.167511: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "2019-08-18 01:22:36.182237: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-08-18 01:22:36.182574: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x24cec40 executing computations on platform Host. Devices:\n",
            "2019-08-18 01:22:36.182602: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-08-18 01:22:36.308173: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-08-18 01:22:36.314773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-18 01:22:36.315203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-08-18 01:22:36.315227: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\n",
            "2019-08-18 01:22:36.315295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-18 01:22:36.315708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-18 01:22:36.316060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-08-18 01:22:36.911855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-08-18 01:22:36.911910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-08-18 01:22:36.911921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-08-18 01:22:36.912122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-18 01:22:36.912492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-18 01:22:36.912829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-08-18 01:22:36.913171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11136 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2019-08-18 01:22:36.915172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x11dcad80 executing computations on platform CUDA. Devices:\n",
            "2019-08-18 01:22:36.915202: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-08-18 01:22:37.601471: W tensorflow/core/framework/allocator.cc:107] Allocation of 31360000 exceeds 10% of system memory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iyht3u6FUZp9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6eee1270-23ea-4e63-c288-20fc8b0da3bc"
      },
      "source": [
        "let (n,m) = (xTrain.shape[0],xTrain.shape[1])\n",
        "let c = yTrain.max().scalarized()+1\n",
        "print(n,m,c)\n",
        "let nHid = 50"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 784 10\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URpjbnZAUcQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "// export\n",
        "public struct BasicModel: Layer {\n",
        "    public var layer1, layer2: FADense<Float>\n",
        "    \n",
        "    public init(nIn: Int, nHid: Int, nOut: Int){\n",
        "        layer1 = FADense(nIn, nHid, activation: relu)\n",
        "        layer2 = FADense(nHid, nOut)\n",
        "    }\n",
        "    \n",
        "    @differentiable\n",
        "    public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "        return layer2(layer1(input))\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1wtvZZpUhh-",
        "colab_type": "text"
      },
      "source": [
        "We can also directly define our model as an array of FADense layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_7wM41sUetm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "var model: [FADense<Float>] = [\n",
        "    FADense(m, nHid, activation: relu),\n",
        "    FADense(nHid, Int(c))] // BasicModel(nIn: m, nHid: nHid, nOut: Int(c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni1KuagRbL4w",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Dataset/DataBunch\n",
        "\n",
        "We add our own wrapper above the S4TF Dataset for several reasons:\n",
        "\n",
        "    in S4TF, Dataset has no length and we need a count property to be able to do efficient hyper-parameters scheduling.\n",
        "    you can only apply batched once to a Dataset but we sometimes want to change the batch size. We save the original non-batched datasetin innerDs.\n",
        "    the shuffle needs to be called each time we want to reshuffle, so we make this happen in the compute property ds.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSKYC81RUjnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "//export \n",
        "public struct FADataset<Element> where Element: TensorGroup {\n",
        "    public var innerDs: Dataset<Element>\n",
        "    public var shuffle = false\n",
        "    public var bs = 64 \n",
        "    public var dsCount: Int\n",
        "    \n",
        "    public var count: Int {\n",
        "        return dsCount%bs == 0 ? dsCount/bs : dsCount/bs+1\n",
        "    }\n",
        "    \n",
        "    public var ds: Dataset<Element> { \n",
        "        if !shuffle { return innerDs.batched(bs)}\n",
        "        let seed = Int64.random(in: Int64.min..<Int64.max)\n",
        "        return innerDs.shuffled(sampleCount: dsCount, randomSeed: seed).batched(bs)\n",
        "    }\n",
        "    \n",
        "    public init(_ ds: Dataset<Element>, len: Int, shuffle: Bool = false, bs: Int = 64) {\n",
        "        (self.innerDs,self.dsCount,self.shuffle,self.bs) = (ds, len, shuffle, bs)\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9P0FxjfbpkG",
        "colab_type": "text"
      },
      "source": [
        "Then we can define a DataBunch to group our training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_I2DtiFbggW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "// export\n",
        "public struct DataBunch<Element> where Element: TensorGroup{\n",
        "    public var train, valid: FADataset<Element>\n",
        "    \n",
        "    public init(train: Dataset<Element>, valid: Dataset<Element>, trainLen: Int, validLen: Int, bs: Int = 64) {\n",
        "        self.train = FADataset(train, len: trainLen, shuffle: true,  bs: bs)\n",
        "        self.valid = FADataset(valid, len: validLen, shuffle: false, bs: 2*bs)\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQIG-yRUbvXF",
        "colab_type": "text"
      },
      "source": [
        "And add a convenience function to get MNIST in a DataBunch directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p69MHJdTbsY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "//export\n",
        "public func mnistDataBunch(path: Path = mnistPath, flat: Bool = false, bs: Int = 64)\n",
        "   -> DataBunch<DataBatch<TF, TI>> {\n",
        "    let (xTrain,yTrain,xValid,yValid) = loadMNIST(path: path, flat: flat)\n",
        "    return DataBunch(train: Dataset(elements: DataBatch(xb:xTrain, yb: yTrain)), \n",
        "                     valid: Dataset(elements: DataBatch(xb:xValid, yb: yValid)),\n",
        "                     trainLen: xTrain.shape[0],\n",
        "                     validLen: xValid.shape[0],\n",
        "                     bs: bs)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_M3L7zZbxsf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "c7407d88-298d-481e-de1b-c67708e4d274"
      },
      "source": [
        "let data = mnistDataBunch(flat: true)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-08-18 01:55:15.257613: W tensorflow/core/framework/allocator.cc:107] Allocation of 188160000 exceeds 10% of system memory.\n",
            "2019-08-18 01:55:16.071856: W tensorflow/core/framework/allocator.cc:107] Allocation of 31360000 exceeds 10% of system memory.\n",
            "2019-08-18 01:55:16.095300: W tensorflow/core/framework/allocator.cc:107] Allocation of 188160000 exceeds 10% of system memory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkPdH38ib4EI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "94de25f7-747e-41cd-aede-86ecafcab942"
      },
      "source": [
        "data.train.count"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "938\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJcf3tmib-dG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Shuffle test\n",
        "\n",
        "Timing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FinyWyWfb8nd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "//export\n",
        "public extension Sequence {\n",
        "  func first() -> Element? {\n",
        "    return first(where: {_ in true})\n",
        "  }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em409pbZcA4D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "63669d3d-5f5d-4755-84a9-715c9bc2d94b"
      },
      "source": [
        "time(repeating: 10) {\n",
        "  let tst = data.train.ds\n",
        "\n",
        "  tst.first()!.yb\n",
        "}"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average: 220.97831200000002 ms,   min: 162.505452 ms,   max: 255.3457 ms\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMUJaZ2ocEaa",
        "colab_type": "text"
      },
      "source": [
        "Check we get difference batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e4ZQ3L2cCgH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "1e55b338-16e0-4b4f-af8d-67a4bc6c8c6e"
      },
      "source": [
        "var tst = data.train.ds\n",
        "tst.first()!.yb"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 2, 5, 3, 4, 6, 6, 9, 5, 6, 8, 6, 6, 3, 1, 6, 3, 7, 3, 9, 2, 9, 7, 9, 5, 7, 2, 7, 1, 7, 0, 9, 9, 8, 1, 2, 5, 3, 6, 2, 1, 5, 0, 6, 8, 2, 0, 8, 6, 0, 8, 5, 1, 7, 5, 3, 3, 1, 0, 9, 2, 1, 3]\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcBmtLB7cI1k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "310e77ba-0490-4ae1-d01a-09297556b298"
      },
      "source": [
        "tst = data.train.ds\n",
        "tst.first()!.yb"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 0, 6, 2, 6, 7, 0, 5, 5, 4, 3, 5, 6, 7, 4, 5, 6, 8, 7, 7, 4, 7, 1, 7, 6, 1, 4, 5, 9, 3, 9, 8, 0, 0, 2, 8, 1, 4, 3, 9, 8, 2, 9, 9, 4, 7, 5, 4, 0, 8, 5, 5, 5, 9, 0, 1, 5, 0, 6, 3, 3, 7, 8, 8]\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aVRvuIOcQ09",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Learner, LearnerAction: enums and error handling in Swift, oh my!\n",
        "\n",
        "Just like in Python, we'll use \"exception handling\" to let custom actions indicate that they want to stop, skip over a batch or do other custom processing - e.g. for early stopping.\n",
        "\n",
        "We'll start by defining a custom type to represent the stop reason, and we'll use a Swift enum to describe it:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aln2uuPXcLmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "// export\n",
        "public enum LearnerAction: Error {\n",
        "    case skipEpoch(reason: String)\n",
        "    case skipBatch(reason: String)\n",
        "    case stop(reason: String)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBeyDH1zcchz",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Now this a bit of an unusual thing - we have met protocols before, and : Error is a protocol that LearnerAction conforms to, but what is going on with those cases?\n",
        "\n",
        "Let's jump briefly into slides to talk about Swift enums:\n",
        "\n",
        "Slides: Supercharged Enums in Swift\n",
        "\n",
        "Basic Learner class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-vNQB73cac1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "// export\n",
        "/// Initializes and trains a model on a given dataset.\n",
        "public final class Learner<Label: TensorGroup,\n",
        "                           Opt: TensorFlow.Optimizer & AnyObject>\n",
        "    where Opt.Scalar: Differentiable,\n",
        "          Opt.Model: Layer,\n",
        "          // Constrain model input to Tensor<Float>, to work around\n",
        "          // https://forums.fast.ai/t/fix-ad-crash-in-learner/42970.\n",
        "          Opt.Model.Input == Tensor<Float>\n",
        "{\n",
        "    public typealias Model = Opt.Model\n",
        "    public typealias Input = Model.Input\n",
        "    public typealias Output = Model.Output\n",
        "    public typealias Data = DataBunch<DataBatch<Input, Label>>\n",
        "    public typealias Loss = TF\n",
        "    public typealias Optimizer = Opt\n",
        "    public typealias Variables = Model.AllDifferentiableVariables\n",
        "    public typealias EventHandler = (Learner) throws -> Void\n",
        "    \n",
        "    /// A wrapper class to hold the loss function, to work around\n",
        "    // https://forums.fast.ai/t/fix-ad-crash-in-learner/42970.\n",
        "    public final class LossFunction {\n",
        "        public typealias F = @differentiable (Model.Output, @nondiff Label) -> Loss\n",
        "        public var f: F\n",
        "        init(_ f: @escaping F) { self.f = f }\n",
        "    }\n",
        "    \n",
        "    public var data: Data\n",
        "    public var opt: Optimizer\n",
        "    public var lossFunc: LossFunction\n",
        "    public var model: Model\n",
        "    \n",
        "    public var currentInput: Input!\n",
        "    public var currentTarget: Label!\n",
        "    public var currentOutput: Output!\n",
        "    \n",
        "    public private(set) var epochCount = 0\n",
        "    public private(set) var currentEpoch = 0\n",
        "    public private(set) var currentGradient = Model.TangentVector.zero\n",
        "    public private(set) var currentLoss = Loss.zero\n",
        "    public private(set) var inTrain = false\n",
        "    public private(set) var pctEpochs = Float.zero\n",
        "    public private(set) var currentIter = 0\n",
        "    public private(set) var iterCount = 0\n",
        "    \n",
        "    open class Delegate {\n",
        "        open var order: Int { return 0 }\n",
        "        public init () {}\n",
        "        \n",
        "        open func trainingWillStart(learner: Learner) throws {}\n",
        "        open func trainingDidFinish(learner: Learner) throws {}\n",
        "        open func epochWillStart(learner: Learner) throws {}\n",
        "        open func epochDidFinish(learner: Learner) throws {}\n",
        "        open func validationWillStart(learner: Learner) throws {}\n",
        "        open func batchWillStart(learner: Learner) throws {}\n",
        "        open func batchDidFinish(learner: Learner) throws {}\n",
        "        open func didProduceNewGradient(learner: Learner) throws {}\n",
        "        open func optimizerDidUpdate(learner: Learner) throws {}\n",
        "        open func batchSkipped(learner: Learner, reason:String) throws {}\n",
        "        open func epochSkipped(learner: Learner, reason:String) throws {}\n",
        "        open func trainingStopped(learner: Learner, reason:String) throws {}\n",
        "        ///\n",
        "        /// TODO: learnerDidProduceNewOutput and learnerDidProduceNewLoss need to\n",
        "        /// be differentiable once we can have the loss function inside the Learner\n",
        "    }\n",
        "    \n",
        "    public var delegates: [Delegate] = [] {\n",
        "        didSet { delegates.sort { $0.order < $1.order } }\n",
        "    }\n",
        "    \n",
        "    public init(data: Data, lossFunc: @escaping LossFunction.F,\n",
        "                optFunc: (Model) -> Optimizer, modelInit: ()->Model) {\n",
        "        (self.data,self.lossFunc) = (data,LossFunction(lossFunc))\n",
        "        model = modelInit()\n",
        "        opt = optFunc(self.model)\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsD-dM_EdEx1",
        "colab_type": "text"
      },
      "source": [
        "Then let's write the parts of the training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcJ5gKSgcssz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "// export\n",
        "extension Learner {\n",
        "    private func evaluate(onBatch batch: DataBatch<Input, Label>) throws {\n",
        "        currentOutput = model(currentInput)\n",
        "        currentLoss = lossFunc.f(currentOutput, currentTarget)\n",
        "    }\n",
        "    \n",
        "    private func train(onBatch batch: DataBatch<Input, Label>) throws {\n",
        "        let (xb,yb) = (currentInput!,currentTarget!) //We still have to force-unwrap those for AD...\n",
        "        (currentLoss, currentGradient) = model.valueWithGradient { model -> Loss in \n",
        "            let y = model(xb)                                      \n",
        "            self.currentOutput = y\n",
        "            return self.lossFunc.f(y, yb)\n",
        "        }\n",
        "        for d in delegates { try d.didProduceNewGradient(learner: self) }\n",
        "        opt.update(&model.variables, along: self.currentGradient)\n",
        "    }\n",
        "    \n",
        "    private func train(onDataset ds: FADataset<DataBatch<Input, Label>>) throws {\n",
        "        iterCount = ds.count\n",
        "        for batch in ds.ds {\n",
        "            (currentInput, currentTarget) = (batch.xb, batch.yb)\n",
        "            do {\n",
        "                for d in delegates { try d.batchWillStart(learner: self) }\n",
        "                if inTrain { try train(onBatch: batch) } else { try evaluate(onBatch: batch) }\n",
        "            }\n",
        "            catch LearnerAction.skipBatch(let reason) {\n",
        "                for d in delegates {try d.batchSkipped(learner: self, reason:reason)}\n",
        "            }\n",
        "            for d in delegates { try d.batchDidFinish(learner: self) }\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grFsQFojdJmH",
        "colab_type": "text"
      },
      "source": [
        "And the whole fit function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxiSTsuAdIXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "// export\n",
        "extension Learner {\n",
        "    /// Starts fitting.\n",
        "    /// - Parameter epochCount: The number of epochs that will be run.\n",
        "    public func fit(_ epochCount: Int) throws {\n",
        "        self.epochCount = epochCount\n",
        "        do {\n",
        "            for d in delegates { try d.trainingWillStart(learner: self) }\n",
        "            for i in 0..<epochCount {\n",
        "                self.currentEpoch = i\n",
        "                do {\n",
        "                    for d in delegates { try d.epochWillStart(learner: self) }\n",
        "                    try train(onDataset: data.train)\n",
        "                    for d in delegates { try d.validationWillStart(learner: self) }\n",
        "                    try train(onDataset: data.valid)\n",
        "                } catch LearnerAction.skipEpoch(let reason) {\n",
        "                    for d in delegates {try d.epochSkipped(learner: self, reason:reason)}\n",
        "                }\n",
        "                for d in delegates { try d.epochDidFinish(learner: self) }\n",
        "            }\n",
        "        } catch LearnerAction.stop(let reason) {\n",
        "            for d in delegates {try d.trainingStopped(learner: self, reason:reason)}\n",
        "        }\n",
        "\n",
        "        for d in delegates { try d.trainingDidFinish(learner: self) }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp8SaIOcdWYe",
        "colab_type": "text"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvgQR0LadVZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func optFunc(_ model: BasicModel) ->  SGD<BasicModel> { return SGD(for: model, learningRate: 1e-2)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N68I6Od5dZR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func modelInit() -> BasicModel {return BasicModel(nIn: m, nHid: nHid, nOut: Int(c))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlyaVdpLdbGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noqBv8Wgdcbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.fit(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eonn5h9bdxba",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Let's add Callbacks!\n",
        "\n",
        "Extension with convenience methods to add delegates:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kafMkJMCdejR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "// export\n",
        "public extension Learner {\n",
        "    func addDelegate (_ delegate :  Learner.Delegate ) { delegates.append(delegate) }\n",
        "    func addDelegates(_ delegates: [Learner.Delegate]) { self.delegates += delegates }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeNKJQX7d1Ud",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Train/eval\n",
        "\n",
        "Callback classes are defined as extensions of the Learner.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0hL3yehdzxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "// export\n",
        "extension Learner {\n",
        "    public class TrainEvalDelegate: Delegate {\n",
        "        public override func trainingWillStart(learner: Learner) {\n",
        "            learner.pctEpochs = 0.0\n",
        "        }\n",
        "\n",
        "        public override func epochWillStart(learner: Learner) {\n",
        "            Context.local.learningPhase = .training\n",
        "            (learner.pctEpochs,learner.inTrain,learner.currentIter) = (Float(learner.currentEpoch),true,0)\n",
        "        }\n",
        "        \n",
        "        public override func batchDidFinish(learner: Learner) {\n",
        "            learner.currentIter += 1\n",
        "            if learner.inTrain{ learner.pctEpochs += 1.0 / Float(learner.iterCount) }\n",
        "        }\n",
        "        \n",
        "        public override func validationWillStart(learner: Learner) {\n",
        "            Context.local.learningPhase = .inference\n",
        "            learner.inTrain = false\n",
        "            learner.currentIter = 0\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    public func makeTrainEvalDelegate() -> TrainEvalDelegate { return TrainEvalDelegate() }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnr0fahKd4dJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwe6iahqd6PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.delegates = [learner.makeTrainEvalDelegate()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JEwkwd4d79y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.fit(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy5alLE9d_44",
        "colab_type": "text"
      },
      "source": [
        "AverageMetric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOrtaxbmd9qY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "// export\n",
        "extension Learner {\n",
        "    public class AvgMetric: Delegate {\n",
        "        public let metrics: [(Output, Label) -> TF]\n",
        "        var total: Int = 0\n",
        "        var partials = [TF]()\n",
        "        \n",
        "        public init(metrics: [(Output, Label) -> TF]) { self.metrics = metrics}\n",
        "        \n",
        "        public override func epochWillStart(learner: Learner) {\n",
        "            total = 0\n",
        "            partials = Array(repeating: Tensor(0), count: metrics.count + 1)\n",
        "        }\n",
        "        \n",
        "        public override func batchDidFinish(learner: Learner) {\n",
        "            if !learner.inTrain{\n",
        "                let bs = learner.currentInput!.shape[0] //Possible because Input is TF for now\n",
        "                total += bs\n",
        "                partials[0] += Float(bs) * learner.currentLoss\n",
        "                for i in 1...metrics.count{\n",
        "                    partials[i] += Float(bs) * metrics[i-1](learner.currentOutput!, learner.currentTarget!)\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        public override func epochDidFinish(learner: Learner) {\n",
        "            for i in 0...metrics.count {partials[i] = partials[i] / Float(total)}\n",
        "            print(\"Epoch \\(learner.currentEpoch): \\(partials)\")\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    public func makeAvgMetric(metrics: [(Output, Label) -> TF]) -> AvgMetric{\n",
        "        return AvgMetric(metrics: metrics)\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "196LuhO2eECJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RpZs1X3eF2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.delegates = [learner.makeTrainEvalDelegate(), learner.makeAvgMetric(metrics: [accuracy])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oPKEnzoeHh5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "06d56db5-7250-49b4-80c4-55bfaba8d9d2"
      },
      "source": [
        "learner.fit(2)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: [0.51169425, 0.8788]\n",
            "Epoch 1: [0.37575504, 0.8999]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuWumVw4eMTT",
        "colab_type": "text"
      },
      "source": [
        "Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMO7iPlLeI1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "// export\n",
        "extension Learner {\n",
        "    public class Normalize: Delegate {\n",
        "        public let mean, std: TF\n",
        "        public init(mean: TF, std: TF) { (self.mean,self.std) = (mean,std) }\n",
        "        \n",
        "        public override func batchWillStart(learner: Learner) {\n",
        "            learner.currentInput = (learner.currentInput! - mean) / std\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    public func makeNormalize(mean: TF, std: TF) -> Normalize{\n",
        "        return Normalize(mean: mean, std: std)\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzKEgUZgeQQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "35c9dcf4-e853-4347-d41c-ca87743e3058"
      },
      "source": [
        "(mean: xTrain.mean(), std: xTrain.standardDeviation())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "▿ 2 elements\n",
              "  - mean : 0.13066049\n",
              "  - std : 0.3081078\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA3_GUU-eShW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "// export\n",
        "public let mnistStats = (mean: TF(0.13066047), std: TF(0.3081079))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox95dGlUeULg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXz9UlHueVvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.delegates = [learner.makeTrainEvalDelegate(), learner.makeAvgMetric(metrics: [accuracy]),\n",
        "                     learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfDBnbR0eXhg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "33a60486-aeb1-4335-b005-2808deaf32a2"
      },
      "source": [
        "learner.fit(2)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: [0.31032494, 0.9114]\n",
            "Epoch 1: [0.25494638, 0.9284]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paDTC3NgecSw",
        "colab_type": "text"
      },
      "source": [
        "Export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCG6sEFOeZBO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8fa00aa2-6ee1-4cef-8a34-ec2044a1695a"
      },
      "source": [
        "import NotebookExport\n",
        "let exporter = NotebookExport(Path.cwd/\"04_callbacks.ipynb\")\n",
        "print(exporter.export(usingPrefix: \"FastaiNotebook_\"))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "success\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNM0BPhRefAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}