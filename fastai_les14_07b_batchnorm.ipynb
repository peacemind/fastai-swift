{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fastai-les14-07b-batchnorm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "swift",
      "display_name": "Swift"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZRlD4utdPuX",
        "colab_type": "code",
        "outputId": "ae73f0b7-3c89-4a76-f352-569aabf34981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%install '.package(path: \"$cwd/FastaiNotebook_06_cuda\")' FastaiNotebook_06_cuda"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing packages:\n",
            "\t.package(path: \"/content/FastaiNotebook_06_cuda\")\n",
            "\t\tFastaiNotebook_06_cuda\n",
            "With SwiftPM flags: []\n",
            "Working in: /tmp/tmp3sf137q5/swift-install\n",
            "Fetching https://github.com/mxcl/Path.swift\n",
            "Fetching https://github.com/saeta/Just\n",
            "Fetching https://github.com/latenitesoft/NotebookExport\n",
            "Completed resolution in 7.16s\n",
            "Cloning https://github.com/saeta/Just\n",
            "Resolving https://github.com/saeta/Just at 0.7.3\n",
            "Cloning https://github.com/latenitesoft/NotebookExport\n",
            "Resolving https://github.com/latenitesoft/NotebookExport at 0.6.0\n",
            "Cloning https://github.com/mxcl/Path.swift\n",
            "Resolving https://github.com/mxcl/Path.swift at 0.16.3\n",
            "[1/11] Compiling Just Just.swift\n",
            "[2/11] Compiling Path Path+Attributes.swift\n",
            "[3/11] Compiling Path Path+Codable.swift\n",
            "[4/11] Compiling Path Path+CommonDirectories.swift\n",
            "[5/11] Compiling Path Path+FileManager.swift\n",
            "[6/11] Compiling Path Path+StringConvertibles.swift\n",
            "[7/11] Compiling Path Path+ls.swift\n",
            "[8/11] Compiling Path Path->Bool.swift\n",
            "[9/11] Compiling Path Path.swift\n",
            "[10/12] Merging module Path\n",
            "[11/16] Compiling NotebookExport DependencyDescription.swift\n",
            "[12/16] Compiling NotebookExport ExtensionUtils.swift\n",
            "[14/17] Merging module Just\n",
            "[15/17] Compiling NotebookExport NotebookExport.swift\n",
            "[16/17] Compiling NotebookExport PackageManifest.swift\n",
            "[17/18] Merging module NotebookExport\n",
            "[18/19] Compiling FastaiNotebook_00_load_data 00_load_data.swift\n",
            "[19/20] Merging module FastaiNotebook_00_load_data\n",
            "[20/22] Compiling FastaiNotebook_01_matmul 01_matmul.swift\n",
            "[21/22] Compiling FastaiNotebook_01_matmul 00_load_data.swift\n",
            "[22/23] Merging module FastaiNotebook_01_matmul\n",
            "[23/26] Compiling FastaiNotebook_01a_fastai_layers 00_load_data.swift\n",
            "[24/26] Compiling FastaiNotebook_01a_fastai_layers 01_matmul.swift\n",
            "[25/26] Compiling FastaiNotebook_01a_fastai_layers 01a_fastai_layers.swift\n",
            "[26/27] Merging module FastaiNotebook_01a_fastai_layers\n",
            "[27/31] Compiling FastaiNotebook_02_fully_connected 00_load_data.swift\n",
            "[28/31] Compiling FastaiNotebook_02_fully_connected 01_matmul.swift\n",
            "[29/31] Compiling FastaiNotebook_02_fully_connected 01a_fastai_layers.swift\n",
            "[30/31] Compiling FastaiNotebook_02_fully_connected 02_fully_connected.swift\n",
            "[31/32] Merging module FastaiNotebook_02_fully_connected\n",
            "[32/37] Compiling FastaiNotebook_02a_why_sqrt5 02_fully_connected.swift\n",
            "[33/37] Compiling FastaiNotebook_02a_why_sqrt5 02a_why_sqrt5.swift\n",
            "[34/37] Compiling FastaiNotebook_02a_why_sqrt5 00_load_data.swift\n",
            "[35/37] Compiling FastaiNotebook_02a_why_sqrt5 01_matmul.swift\n",
            "[36/37] Compiling FastaiNotebook_02a_why_sqrt5 01a_fastai_layers.swift\n",
            "[37/38] Merging module FastaiNotebook_02a_why_sqrt5\n",
            "[38/44] Compiling FastaiNotebook_03_minibatch_training 02_fully_connected.swift\n",
            "[39/44] Compiling FastaiNotebook_03_minibatch_training 02a_why_sqrt5.swift\n",
            "[40/44] Compiling FastaiNotebook_03_minibatch_training 03_minibatch_training.swift\n",
            "[41/44] Compiling FastaiNotebook_03_minibatch_training 00_load_data.swift\n",
            "[42/44] Compiling FastaiNotebook_03_minibatch_training 01_matmul.swift\n",
            "[43/44] Compiling FastaiNotebook_03_minibatch_training 01a_fastai_layers.swift\n",
            "[44/45] Merging module FastaiNotebook_03_minibatch_training\n",
            "[45/52] Compiling FastaiNotebook_04_callbacks 02a_why_sqrt5.swift\n",
            "[46/52] Compiling FastaiNotebook_04_callbacks 03_minibatch_training.swift\n",
            "[47/52] Compiling FastaiNotebook_04_callbacks 04_callbacks.swift\n",
            "[48/52] Compiling FastaiNotebook_04_callbacks 00_load_data.swift\n",
            "[49/52] Compiling FastaiNotebook_04_callbacks 01_matmul.swift\n",
            "[50/52] Compiling FastaiNotebook_04_callbacks 01a_fastai_layers.swift\n",
            "[51/52] Compiling FastaiNotebook_04_callbacks 02_fully_connected.swift\n",
            "[52/53] Merging module FastaiNotebook_04_callbacks\n",
            "[53/61] Compiling FastaiNotebook_05_anneal 02a_why_sqrt5.swift\n",
            "[54/61] Compiling FastaiNotebook_05_anneal 03_minibatch_training.swift\n",
            "[55/61] Compiling FastaiNotebook_05_anneal 04_callbacks.swift\n",
            "[56/61] Compiling FastaiNotebook_05_anneal 05_anneal.swift\n",
            "[57/61] Compiling FastaiNotebook_05_anneal 00_load_data.swift\n",
            "[58/61] Compiling FastaiNotebook_05_anneal 01_matmul.swift\n",
            "[59/61] Compiling FastaiNotebook_05_anneal 01a_fastai_layers.swift\n",
            "[60/61] Compiling FastaiNotebook_05_anneal 02_fully_connected.swift\n",
            "[61/62] Merging module FastaiNotebook_05_anneal\n",
            "[62/71] Compiling FastaiNotebook_05b_early_stopping 03_minibatch_training.swift\n",
            "[63/71] Compiling FastaiNotebook_05b_early_stopping 04_callbacks.swift\n",
            "[64/71] Compiling FastaiNotebook_05b_early_stopping 05_anneal.swift\n",
            "[65/71] Compiling FastaiNotebook_05b_early_stopping 05b_early_stopping.swift\n",
            "[66/71] Compiling FastaiNotebook_05b_early_stopping 00_load_data.swift\n",
            "[67/71] Compiling FastaiNotebook_05b_early_stopping 01_matmul.swift\n",
            "[68/71] Compiling FastaiNotebook_05b_early_stopping 01a_fastai_layers.swift\n",
            "[69/71] Compiling FastaiNotebook_05b_early_stopping 02_fully_connected.swift\n",
            "[70/71] Compiling FastaiNotebook_05b_early_stopping 02a_why_sqrt5.swift\n",
            "[71/72] Merging module FastaiNotebook_05b_early_stopping\n",
            "[72/82] Compiling FastaiNotebook_06_cuda 03_minibatch_training.swift\n",
            "[73/82] Compiling FastaiNotebook_06_cuda 04_callbacks.swift\n",
            "[74/82] Compiling FastaiNotebook_06_cuda 05_anneal.swift\n",
            "[75/82] Compiling FastaiNotebook_06_cuda 05b_early_stopping.swift\n",
            "[76/82] Compiling FastaiNotebook_06_cuda 06_cuda.swift\n",
            "[77/82] Compiling FastaiNotebook_06_cuda 00_load_data.swift\n",
            "[78/82] Compiling FastaiNotebook_06_cuda 01_matmul.swift\n",
            "[79/82] Compiling FastaiNotebook_06_cuda 01a_fastai_layers.swift\n",
            "[80/82] Compiling FastaiNotebook_06_cuda 02_fully_connected.swift\n",
            "[81/82] Compiling FastaiNotebook_06_cuda 02a_why_sqrt5.swift\n",
            "[82/83] Merging module FastaiNotebook_06_cuda\n",
            "[83/84] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
            "[84/85] Merging module jupyterInstalledPackages\n",
            "[85/85] Linking libjupyterInstalledPackages.so\n",
            "Initializing Swift...\n",
            "Installation complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPqe3HaqEiqt",
        "colab_type": "code",
        "outputId": "a3a9c8c5-bcb1-4c62-d72b-7d975341d5a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import FastaiNotebook_06_cuda\n",
        "%include \"EnableIPythonDisplay.swift\"\n",
        "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('inline', 'module://ipykernel.pylab.backend_inline')\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzFH3ntUEKqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "//export\n",
        "import Path\n",
        "import TensorFlow\n",
        "import Python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlHAHzocElaL",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Let's start by building our own batchnorm layer from scratch. Eventually we want something like this to work:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1LpMe-WEkZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlmostBatchNorm<Scalar: TensorFlowFloatingPoint> { // : Layer\n",
        "    // Configuration hyperparameters\n",
        "    let momentum, epsilon: Scalar\n",
        "    // Running statistics\n",
        "    var runningMean, runningVariance: Tensor<Scalar>\n",
        "    // Trainable parameters\n",
        "    var scale, offset: Tensor<Scalar>\n",
        "    \n",
        "    init(featureCount: Int, momentum: Scalar = 0.9, epsilon: Scalar = 1e-5) {\n",
        "        (self.momentum, self.epsilon) = (momentum, epsilon)\n",
        "        (scale, offset) = (Tensor(ones: [featureCount]), Tensor(zeros: [featureCount]))\n",
        "        (runningMean, runningVariance) = (Tensor(0), Tensor(1))\n",
        "    }\n",
        "\n",
        "    func call(_ input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
        "        let mean, variance: Tensor<Scalar>\n",
        "        switch Context.local.learningPhase {\n",
        "        case .training:\n",
        "            mean = input.mean(alongAxes: [0, 1, 2])\n",
        "            variance = input.variance(alongAxes: [0, 1, 2])\n",
        "            runningMean += (mean - runningMean) * (1 - momentum)\n",
        "            runningVariance += (variance - runningVariance) * (1 - momentum)\n",
        "        case .inference:\n",
        "            (mean, variance) = (runningMean, runningVariance)\n",
        "        }\n",
        "        let normalizer = rsqrt(variance + epsilon) * scale\n",
        "        return (input - mean) * normalizer + offset\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0ZXPBvYEsEW",
        "colab_type": "text"
      },
      "source": [
        "But there are some automatic differentiation limitations (lack of support for classes and control flow) that make this impossible for now, so we'll need a few workarounds. A Reference will let us update running statistics without making the layer a class or declaring the applied method mutating:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWzu6XpEpdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "//export\n",
        "public class Reference<T> {\n",
        "    public var value: T\n",
        "    public init(_ value: T) { self.value = value }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BawxLcdREwbH",
        "colab_type": "text"
      },
      "source": [
        "The following snippet will let us differentiate a layer's forward method (which is the one called in call for FALayer) if it's composed of training and inference implementations that are each differentiable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPW9jMdMEuOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "//export\n",
        "public protocol LearningPhaseDependent: FALayer {\n",
        "    associatedtype Input\n",
        "    associatedtype Output\n",
        "    @differentiable func forwardTraining (_ input: Input) -> Output\n",
        "    @differentiable func forwardInference(_ input: Input) -> Output\n",
        "}\n",
        "\n",
        "extension LearningPhaseDependent {\n",
        "    public func forward(_ input: Input) -> Output {\n",
        "        switch Context.local.learningPhase {\n",
        "        case .training:  return forwardTraining(input)\n",
        "        case .inference: return forwardInference(input)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    @differentiating(forward)\n",
        "    func gradForward(_ input: Input) ->\n",
        "        (value: Output, pullback: (Self.Output.TangentVector) ->\n",
        "            (Self.TangentVector, Self.Input.TangentVector)) {\n",
        "        switch Context.local.learningPhase {\n",
        "        case .training:  return valueWithPullback(at: input) { $0.forwardTraining($1)  }\n",
        "        case .inference: return valueWithPullback(at: input) { $0.forwardInference($1) }\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmeevemhE03t",
        "colab_type": "text"
      },
      "source": [
        "Now we can implement a BatchNorm that we can use in our models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyvYxPhuEy_V",
        "colab_type": "code",
        "outputId": "853eb5d9-7c68-4d8c-a795-960c9082422a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "//export\n",
        "public protocol Norm: FALayer where Input == TF, Output == TF {\n",
        "    init(_ featureCount: Int, epsilon: Float)\n",
        "}\n",
        "\n",
        "public struct FABatchNorm: LearningPhaseDependent, Norm {\n",
        "    // Configuration hyperparameters\n",
        "    @noDerivative var momentum, epsilon: Float\n",
        "    // Running statistics\n",
        "    @noDerivative let runningMean, runningVariance: Reference<TF>\n",
        "    // Trainable parameters\n",
        "    public var scale, offset: TF\n",
        "    \n",
        "    public init(_ featureCount: Int, momentum: Float, epsilon: Float = 1e-5) {\n",
        "        self.momentum = momentum\n",
        "        self.epsilon = epsilon\n",
        "        self.scale = Tensor(ones: [featureCount])\n",
        "        self.offset = Tensor(zeros: [featureCount])\n",
        "        self.runningMean = Reference(Tensor(0))\n",
        "        self.runningVariance = Reference(Tensor(1))\n",
        "    }\n",
        "    \n",
        "    public init(_ featureCount: Int, epsilon: Float = 1e-5) {\n",
        "        self.init(featureCount, momentum: 0.9, epsilon: epsilon)\n",
        "    }\n",
        "\n",
        "    @differentiable\n",
        "    public func forwardTraining(_ input: TF) -> TF {\n",
        "        let mean = input.mean(alongAxes: [0, 1, 2])\n",
        "        let variance = input.variance(alongAxes: [0, 1, 2])\n",
        "        runningMean.value += (mean - runningMean.value) * (1 - momentum)\n",
        "        runningVariance.value += (variance - runningVariance.value) * (1 - momentum)\n",
        "        let normalizer = rsqrt(variance + epsilon) * scale\n",
        "        return (input - mean) * normalizer + offset\n",
        "    }\n",
        "    \n",
        "    @differentiable\n",
        "    public func forwardInference(_ input: TF) -> TF {\n",
        "        let (mean, variance) = (runningMean.value, runningVariance.value)\n",
        "        let normalizer = rsqrt(variance + epsilon) * scale\n",
        "        return (input - mean) * normalizer + offset\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "",
          "evalue": "ignored",
          "traceback": [
            "error: <Cell 8>:6:15: error: type 'FABatchNorm' does not conform to protocol 'FALayer'\npublic struct FABatchNorm: LearningPhaseDependent, Norm {\n              ^\n\nFastaiNotebook_06_cuda.FALayer:2:9: note: protocol requires property 'delegates' with type '[(TF) -> ()]' (aka 'Array<(Tensor<Float>) -> ()>'); do you want to add a stub?\n    var delegates: [(Self.Output) -> ()] { get set }\n        ^\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX2fymOqFH61",
        "colab_type": "text"
      },
      "source": [
        "Here is a generic ConvNorm layer, that combines a conv2d and a norm (like batchnorm, running batchnorm etc...) layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaamOIMWE4Y_",
        "colab_type": "code",
        "outputId": "521288b4-8210-417c-9f51-087ff62abd30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        }
      },
      "source": [
        "//export\n",
        "public struct ConvNorm<NormType: Norm & FALayer>: FALayer\n",
        "    where NormType.AllDifferentiableVariables == NormType.TangentVector {\n",
        "    public var conv: FANoBiasConv2D<Float>\n",
        "    public var norm: NormType\n",
        "    \n",
        "    public init(_ cIn: Int, _ cOut: Int, ks: Int = 3, stride: Int = 2){\n",
        "        self.conv = FANoBiasConv2D(cIn, cOut, ks: ks, stride: stride, activation: relu) \n",
        "        self.norm = NormType(cOut, epsilon: 1e-5)\n",
        "    }\n",
        "\n",
        "    @differentiable\n",
        "    public func forward(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "        return norm(conv(input))\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "",
          "evalue": "ignored",
          "traceback": [
            "error: <Cell 9>:2:34: error: use of undeclared type 'Norm'\npublic struct ConvNorm<NormType: Norm & FALayer>: FALayer\n                                 ^~~~\n\nerror: <Cell 9>:3:20: error: 'AllDifferentiableVariables' is not a member type of 'NormType'\n    where NormType.AllDifferentiableVariables == NormType.TangentVector {\n          ~~~~~~~~ ^\n\nerror: <Cell 9>:3:59: error: 'TangentVector' is not a member type of 'NormType'\n    where NormType.AllDifferentiableVariables == NormType.TangentVector {\n                                                 ~~~~~~~~ ^\n\nerror: <Cell 9>:2:15: error: type 'ConvNorm<NormType>' does not conform to protocol 'FALayer'\npublic struct ConvNorm<NormType: Norm & FALayer>: FALayer\n              ^\n\nwarning: <Cell 9>:5:5: warning: stored property 'norm' has no derivative because it does not conform to 'Differentiable'; add an explicit '@noDerivative' attribute\n    public var norm: NormType\n    ^\n    @noDerivative \n\nFastaiNotebook_06_cuda.FALayer:2:9: note: protocol requires property 'delegates' with type '[(Tensor<Float>) -> ()]'; do you want to add a stub?\n    var delegates: [(Self.Output) -> ()] { get set }\n        ^\n\nerror: <Cell 9>:9:21: error: non-nominal type 'NormType' does not support explicit initialization\n        self.norm = NormType(cOut, epsilon: 1e-5)\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nerror: <Cell 9>:14:20: error: cannot call value of non-function type 'NormType'\n        return norm(conv(input))\n               ~~~~^\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJkJMObYFKXm",
        "colab_type": "code",
        "outputId": "11f2e052-00df-4728-93fe-bf0af6a332f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        }
      },
      "source": [
        "//export\n",
        "public struct CnnModelNormed<NormType: Norm & FALayer>: FALayer\n",
        "    where NormType.AllDifferentiableVariables == NormType.TangentVector {\n",
        "    public var convs: [ConvNorm<NormType>]\n",
        "    public var pool = FAGlobalAvgPool2D<Float>()\n",
        "    public var linear: FADense<Float>\n",
        "    \n",
        "    public init(channelIn: Int, nOut: Int, filters: [Int]){\n",
        "        let allFilters = [channelIn] + filters\n",
        "        convs = Array(0..<filters.count).map { i in\n",
        "            return ConvNorm<NormType>(allFilters[i], allFilters[i+1], ks: 3, stride: 2)\n",
        "        }\n",
        "        linear = FADense<Float>(filters.last!, nOut)\n",
        "    }\n",
        "    \n",
        "    @differentiable\n",
        "    public func forward(_ input: TF) -> TF {\n",
        "        // TODO: Work around https://bugs.swift.org/browse/TF-606\n",
        "        return linear.forward(pool.forward(convs(input)))\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "",
          "evalue": "ignored",
          "traceback": [
            "error: <Cell 10>:2:40: error: use of undeclared type 'Norm'\npublic struct CnnModelNormed<NormType: Norm & FALayer>: FALayer\n                                       ^~~~\n\nerror: <Cell 10>:3:20: error: 'AllDifferentiableVariables' is not a member type of 'NormType'\n    where NormType.AllDifferentiableVariables == NormType.TangentVector {\n          ~~~~~~~~ ^\n\nerror: <Cell 10>:3:59: error: 'TangentVector' is not a member type of 'NormType'\n    where NormType.AllDifferentiableVariables == NormType.TangentVector {\n                                                 ~~~~~~~~ ^\n\nerror: <Cell 10>:4:24: error: use of undeclared type 'ConvNorm'\n    public var convs: [ConvNorm<NormType>]\n                       ^~~~~~~~\n\nerror: <Cell 10>:2:15: error: type 'CnnModelNormed<NormType>' does not conform to protocol 'FALayer'\npublic struct CnnModelNormed<NormType: Norm & FALayer>: FALayer\n              ^\n\nwarning: <Cell 10>:4:5: warning: stored property 'convs' has no derivative because it does not conform to 'Differentiable'; add an explicit '@noDerivative' attribute\n    public var convs: [ConvNorm<NormType>]\n    ^\n    @noDerivative \n\nFastaiNotebook_06_cuda.FALayer:2:9: note: protocol requires property 'delegates' with type '[(TF) -> ()]' (aka 'Array<(Tensor<Float>) -> ()>'); do you want to add a stub?\n    var delegates: [(Self.Output) -> ()] { get set }\n        ^\n\nerror: <Cell 10>:11:20: error: use of unresolved identifier 'ConvNorm'\n            return ConvNorm<NormType>(allFilters[i], allFilters[i+1], ks: 3, stride: 2)\n                   ^~~~~~~~\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B_nQTqSFWDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}